{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "# logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load cleaned email texts \n",
    "import cPickle \n",
    "f = file('/notebooks/LDA models and data/Data Frames and lists/text_term_matrix_clean.pkl', 'rb')\n",
    "text_term_matrix=cPickle.load(f)\n",
    "\n",
    "text_clean_path = file('/notebooks/LDA models and data/Data Frames and lists/text_clean.pkl', 'rb')\n",
    "text_clean=cPickle.load(text_clean_path)\n",
    "\n",
    "dictfile=file('/notebooks/LDA models and data/Data Frames and lists/Dictionary_tfidf_05V2.pkl', 'rb')\n",
    "Dictionary=cPickle.load(dictfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(21856 unique tokens: [u'chudson', u'polytechniqu', u'suzann', u'francesco', u'ebtd']...)\n"
     ]
    }
   ],
   "source": [
    "print(Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictfile=file('/notebooks/LDA models and data/Data Frames and lists/Dictionary.pkl', 'rb')\n",
    "Dictionary2=cPickle.load(dictfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(18297 unique tokens: [u'setc', u'woodi', u'electrabel', u'yellow', u'doucett']...)\n"
     ]
    }
   ],
   "source": [
    "print(Dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "Dictionary.filter_extremes(no_below=50, no_above=0.1)\n",
    "print(Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_term_matrix0 = [Dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "# apparantly some emails got stripped of all words through our processing; thsi is not strange as emails like \n",
    "#'thnaks' are not unusual\n",
    "import numpy as np\n",
    "k=0\n",
    "for i in range(0, len(text_term_matrix0)):\n",
    "    if len(np.array(text_term_matrix0[i]).shape)==2:\n",
    "        if np.array(text_term_matrix0[i]).shape[1]==2:\n",
    "            k=k+1\n",
    "\n",
    "l=0\n",
    "for i in range(0, len(text_term_matrix0)):\n",
    "    if len(np.array(text_term_matrix0[i]).shape)==1:\n",
    "        if np.array(text_term_matrix0[i]).shape[0]==0:\n",
    "            l=l+1\n",
    "print l            \n",
    "print len(text_term_matrix0)\n",
    "print len(text_term_matrix0)-k\n",
    "\n",
    "# hier gebleven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first cretae new bags list by removing empty bags\n",
    "text_term_matrix00=[] \n",
    "for i in range(0, len(text_term_matrix0)):\n",
    "    if len(np.array(text_term_matrix0[i]).shape)==2:\n",
    "        if np.array(text_term_matrix0[i]).shape[1]==2:\n",
    "            text_term_matrix00.append(text_term_matrix0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function for evaluation of cossim scores as folows: slpit test portion\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def intra_inter(model, part1, part2, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1_mod = [model[bow] for bow in part1]\n",
    "    part2_mod = [model[bow] for bow in part2]\n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1_mod, part2_mod)]))\n",
    "    intra = np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1_mod, part2_mod)])\n",
    "    \n",
    "    p=len(part1)\n",
    "    random_pairs = np.random.randint(0, p-1, size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1_mod[i[0]], part2_mod[i[1]]) for i in random_pairs]))\n",
    "    inter = np.mean([gensim.matutils.cossim(part1_mod[i[0]], part2_mod[i[1]]) for i in random_pairs])\n",
    "    \n",
    "    \n",
    "    return [intra, inter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "grid_train = defaultdict(list)\n",
    "grid_test = defaultdict(list)\n",
    "cosine_score=[]\n",
    "\n",
    "# num topics list to loop through for LDA output evaluation\n",
    "parameter_list=[10, 20, 30, 50, 60, 75, 100, 125, 150]\n",
    "  \n",
    "# split into 50% training and 50% test sets for the permutation cossim test (see function intra_inter in prevous cell)\n",
    "p_train = int(len(text_term_matrix00) *0.7)\n",
    "rc =np.random.choice(len(text_term_matrix00), size=len(text_term_matrix00), replace=False, p=None)\n",
    "cp_train=[text_term_matrix00[i] for i in rc[0:p_train]]\n",
    "\n",
    "# Sample from corpus for evaluation of perplexity and cossimilarities; evaluation of full set of (even only test) documants (150k) takes too long\n",
    "# Pull out the test part of corpus\n",
    "cp_test_full=[text_term_matrix00[i] for i in rc[p_train:]]\n",
    "# Make a smaller selection as perplexity computation take a lot of time\n",
    "cp_test=random.sample(cp_test_full, 20000)\n",
    "\n",
    "# DSplit each bag of words at random into two parts for cossim valuation\n",
    "cp_test_part1=[]\n",
    "cp_test_part2=[]\n",
    "for i in range(len(cp_test)):\n",
    "    p_test_inner = int(len(cp_test[i]) *0.5)\n",
    "    rc =np.random.choice(len(cp_test[i]), size=len(cp_test[i]), replace=False, p=None)\n",
    "    cp_test_part1.append([cp_test[i] for j in rc[0:p_test_inner]])\n",
    "    cp_test_part2.append([cp_test[i] for j in rc[p_test_inner:]])\n",
    "\n",
    "# Subselect random portion of train bags of owrds for peropoexity valuation\n",
    "cp_train_perplexity_sample=random.sample(cp_train, 20000)\n",
    "\n",
    "# for num_topics_value in num_topics_list:\n",
    "for parameter_value in parameter_list:\n",
    "    # print \"starting pass for num_topic = %d\" % num_topics_value\n",
    "    print \"starting pass for parameter_value = %.3f\" % parameter_value\n",
    "    start_time = time.time()\n",
    "\n",
    "    # run model\n",
    "    model = gensim.models.ldamulticore.LdaMulticore(corpus=cp_train, id2word=Dictionary, num_topics=parameter_value, chunksize=3125,\\\n",
    "                                     passes=1, eval_every=None, alpha=None, eta=None, decay=0.5)\n",
    "    \n",
    "    # show elapsed time for model\n",
    "    elapsed = time.time() - start_time\n",
    "    print \"Elapsed time: %s\" % elapsed\n",
    "    \n",
    "    perplex_test = model.bound(cp_test)\n",
    "    print \"Perplexity test: %s\" % perplex_test\n",
    "    grid_test[parameter_value].append(perplex_test)\n",
    "    \n",
    "    perplex_train = model.bound(cp_train_perplexity_sample)\n",
    "    print \"Perplexity train: %s\" % perplex_train\n",
    "    grid_train[parameter_value].append(perplex_train)\n",
    "    \n",
    "    per_word_perplex_test = np.exp2(-perplex_test / sum(cnt for document in cp_test for _, cnt in document))\n",
    "    print \"Per-word Perplexity test: %s\" % per_word_perplex_test\n",
    "    grid_test[parameter_value].append(per_word_perplex_test)\n",
    "    \n",
    "    per_word_perplex_train = np.exp2(-perplex_train / sum(cnt for document in cp_train for _, cnt in document))\n",
    "    print \"Per-word Perplexity train: %s\" % per_word_perplex_train\n",
    "    grid_test[parameter_value].append(per_word_perplex_train)\n",
    "    \n",
    "    \n",
    "    cosine_score.append(intra_inter(model, cp_test_part1, cp_test_part2, num_pairs=20000))\n",
    "# get `v1 = lsi.projection.u[dictionary.token2id['the']]; v2 = lsi.projection.u[dictionary.token2id['of']]` and print cossim(v1, v2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_term_matrix2 = [Dictionary2.doc2bow(text) for text in text_clean]\n",
    "\n",
    "lda_20topics = gensim.models.ldamulticore.LdaMulticore(corpus=text_term_matrix, id2word=Dictionary_tfidf_05, num_topics=20, chunksize=3125,\\\n",
    "                                     passes=1, eval_every=None, alpha=None, eta=None, decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize LDA model with an beautiful interactive tool :-))\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# pyLDAvis.prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R=30, lambda_step=0.01,\\\n",
    "#                  mds=<function js_PCoA>, n_jobs=-1, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=True)\n",
    "vis20=pyLDAvis.gensim.prepare(lda_20topics, text_term_matrix, Dictionary)\n",
    "pyLDAvis.display(vis20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
