{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Script content\n",
    " \n",
    "### 1) This script downloads df_emails data frame which contains one row per email, with tokenized email body and inferred owner. \n",
    " \n",
    "##### In/out info about the emails follows based on whether folder name matches the name in X-from of email. We will process email bodies in two ways, one for topic modelling and slightly differently for recommendation engine. \n",
    " \n",
    "##### Difference is that for the later version if email contains a forwarded email thread in itâ€™s body, then we need to remove that part keeping only what the sender wrote (outbox emails) or what is been written to him/her in the last email in the thread (inbox emails). It is a choice we are making to infer preferences and expertise levels on topics--with outbox it is quite clear, and in case of inbox this choice seems to be correct as it prevents earlier emails by the folder owner in a thread to be counted for preference levels. \n",
    " \n",
    "##### Furthermore, from each email body we will remove warnings at the bottom of emails (privacy, environment etc) as well as From, To, X-From, X-to and all other fields that come along with a  forward thread. This is done for for both versions of processing.\n",
    "\n",
    "### 2) Then script removes standard English words, digits, spaces and special characters, words with 1 or 2 letters; it bring nouns to singular form, removes suffix where applicable --- with a nested list 'clean_text' and 'clean_text0'  as result. \n",
    "\n",
    "### 3) For persistency we save both lists with cPickle to the disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download de_emails\n",
    "import pandas as pd\n",
    "df_emails=pd.read_pickle('/notebooks/LDA models and data/Data Frames and lists/df_emails.pkl')\n",
    "df_emails0=pd.read_pickle('/notebooks/LDA models and data/Data Frames and lists/df_emails0.pkl')\n",
    "# LDA models and data/Data Frames and lists  df_emails.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "df_emails=df_emails.sample(n=200)\n",
    "df_emails0=df_emails0.sample(n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# LDA\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.lda import LDA\n",
    "# from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to process email bodies to tokenized words sequences. That is we apply the standard steps for statistical processing of natural language. \n",
    "\n",
    "### We first write a function 'clean_text' in the first cell below which removes punctuation and special characters, digits as well as English words which are too common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()) )])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    two_letter_words_free = \" \".join([i for i in stem.split() if len(i)>=3])\n",
    "    \n",
    "#     return stem\n",
    "    return two_letter_words_free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### CHECK how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'demonstr action function work'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(' !This is a 45.98*demonstrating. Of af actions ji of this function\\'s $#wORKing/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next process all bodies into a list. We write two functions, one that only takes email bodies and another that also takes directory path name and in-outbox identifier. This can he handled in a single function but we leave such implementation details for a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for LDA training part and valuation\n",
    "\n",
    "def text_clean_df2list1(data):\n",
    "    text_clean=[]\n",
    "    for text in data['body']:\n",
    "        text_clean.append(clean_text(text).split())\n",
    "    return text_clean\n",
    "    \n",
    "# Function for recommendation engine part\n",
    "    \n",
    "def text_clean_df2list0(data):\n",
    "    text_clean0=[]\n",
    "#     k=0\n",
    "    for k in range(0, data.shape[0]):# dirpath, inoutid, text in data[['dirpath', 'inout_id', 'body']]:\n",
    "        text_clean0.append([data['dirpath'].iloc[k], data['inout_id'].iloc[k], text_clean.append(clean_text(data['body'].\\\n",
    "                                                                                                 iloc[k]).split()  ) ] )\n",
    "    return text_clean0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now process df_emails and df_emails0. Recall that second one has texts from forwarded email threads removed from bodies. Also we will take directory path name as well as in/outbox id as we need it later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above two lists are sufficient to proceed and we save them to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('/notebooks/LDA models and data/Data Frames and lists/text_clean.pkl', 'wb') as pickle_file:\n",
    "    cPickle.dump(obj=text_clean, file=pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('/notebooks/LDA models and data/Data Frames and lists/text_clean0.pkl', 'wb') as pickle_file:\n",
    "cPickle.dump(obj=text_clean01, file=pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
