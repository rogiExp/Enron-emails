{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script downloads df_emails dat frame which cotains one row per email, with cleaned email body and infered owner. In/out\n",
    "# info about the emails follows based on whther folder name matches the name in X-from (dist1=0 or dist2=0)\n",
    "\n",
    "# Then script processes all emails bodies to bags of words, thereby removing standard English words, digits, spaces and special\n",
    "# characters, words with 1 or 2 letters, bring nouns to singular form, removes sufix where appl. with appropriate nested list \n",
    "# as result. \n",
    "\n",
    "# Subsequently we determine tfidf values per word/email combination and remove all words with a tfidf bellow a threshold level\n",
    "# from each single document. Word is removed from dictionary al together only if it haf a low tfidf in IN ALL DOCUMENTS!\n",
    "\n",
    "# We chose the tfidf threshold such that we keep not to many words. Chpice to keep about 15.000 words is a bit arbitrary\n",
    "# at this point. A way to determine Ã³ptimal'number of words is to measure perplexity of our topic model, against\n",
    "# choices of tfidf threshold together with number of topics. Due to run time of lda model we will keep the tfidf threshold\n",
    "# choice at this one level only but we will try different options for number fo topics in our next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_emails=pd.read_pickle('/notebooks/LDA models and data/Data Frames and lists/df_emails.plk')\n",
    "df_emails0=pd.read_pickle('/notebooks/LDA models and data/Data Frames and lists/df_emails0.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns; sns.set_style('whitegrid')\n",
    "# import wordcloud\n",
    "\n",
    "# Network analysis\n",
    "# import networkx as nx\n",
    "\n",
    "\n",
    "# from subprocess import check_output\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# LDA\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function which removes punctuation and special characters, digits as weel as English words which are too common.\n",
    "# In addition \n",
    "def clean_text(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    porter= PorterStemmer()\n",
    "    \n",
    "    text=text.rstrip()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()) )])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    two_letter_words_free = \" \".join([i for i in stem.split() if len(i)>=3])\n",
    "    \n",
    "#     return normalized\n",
    "    return two_letter_words_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_text(' !This is a 45.98*demonstrating. Of af actions ji of this function\\'s $#wORKing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of processed email texts needed to feed into gensim functions\n",
    "text_clean=[]\n",
    "for text in df_emails['body']:\n",
    "    text_clean.append(clean_text(text).split())\n",
    "    \n",
    "# create a list of processed email texts needed to feed into gensim functions\n",
    "# df_emails0=df_emails0.reset_index()\n",
    "text_clean0=[]\n",
    "for text in df_emails0['body']:\n",
    "    text_clean0.append(clean_text(text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of processed email texts needed to feed into gensim functions\n",
    "text_clean=[]\n",
    "for text in df_emails['body']:\n",
    "    text_clean.append(clean_text(text).split())\n",
    "\n",
    "# repeat but now with the data frame processed without fowardd email peices and inclusing directory path name and in/out id\n",
    "text_clean01=[]\n",
    "df_emails01= df_emails0[0:100]\n",
    "k=0\n",
    "for text in text_clean0:\n",
    "    text_clean01.append([df_emails0['dirpath'][k], df_emails0['inout_id'][k], text])\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('/notebooks/LDA models and data/Data Frames and lists/text_cleanV3.pkl', 'wb') as pickle_file:\n",
    "    cPickle.dump(obj=text_clean, file=pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump(obj=text_clean01, file=pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of words (keys) and id's (values) pairs appearing in all emails (the words that we kept)\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "# Create list of bags of words, one for each cleaned email\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first cretae new nags list by removing empty bags\n",
    "text_term_matrix_clean=[]\n",
    "for i in range(0, len(text_term_matrix)):\n",
    "    if len(np.array(text_term_matrix[i]).shape)==2:\n",
    "        if np.array(text_term_matrix[i]).shape[1]==2:\n",
    "            text_term_matrix_clean.append(text_term_matrix[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter low value words\n",
    "low_value = 0.5\n",
    "\n",
    "#instantiate empty array to extract our bag of words list text_term_matrix into an array and idem for low tfidf words.\n",
    "# this will aloow us easily to reduce the decitionary removing only words that are infrequentg in all doecumenmst where\n",
    "# they appear (only! we need to construct a work around like this as all words appear only in some documents)\n",
    "low_value_words_list=[]\n",
    "all_value_words_list=[]\n",
    "# instantiate newbag of words \n",
    "text_term_matrix_tfidf=[]\n",
    "count_reductions=0\n",
    "for i in range(0, len(text_term_matrix_clean)):\n",
    "    bow = text_term_matrix_clean[i]\n",
    "    low_value_words = []\n",
    "    # collect low tfidf value words\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    # collect complementary words \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "    diff_bow = [b for b in bow if b[0] in low_value_words]\n",
    "    # update array of low value words and also of all words-this will allow us by aggretation step to determine which words are \n",
    "    # low tfidf in all bags of words so dictionary can be reduced for those\n",
    "    # we need to check in each step if: any words are low, and if anything is left in the bag.\n",
    "    # We also update new bags list and count how many bags have been reduced, just for checks purpose\n",
    "    if len(np.array(new_bow).shape)==2:\n",
    "        if np.array(new_bow).shape[1]==2: \n",
    "            # update our new bags of words list\n",
    "            text_term_matrix_tfidf.append(new_bow)\n",
    "            \n",
    "    if len(np.array(diff_bow).shape)==2:\n",
    "        if np.array(diff_bow).shape[1]==2:\n",
    "#             low_value_words_array=np.append(arr=low_value_words_array, values=np.array(new_bow), axis=0)\n",
    "            low_value_words_list.append(diff_bow)\n",
    "            count_reductions=count_reductions+1\n",
    "#     all_value_words_array=np.append(arr=all_value_words_array, values=np.array(bow), axis=0)\n",
    "    all_value_words_list.append(bow)\n",
    "print count_reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in this cell we prodcue data frame which contains folowing columns:\n",
    "# wordid as in our dictionary, number of documents in which this word appears, total number of occurencies and total number of\n",
    "# emails where the word doc tfidf has value bellow our threshold\n",
    "\n",
    "# start by transforming lists to appropriate dat frame so we can aggregate\n",
    "all_value_words_array = np.concatenate([np.array(i) for i in all_value_words_list])\n",
    "low_value_words_array = np.concatenate([np.array(i) for i in low_value_words_list])\n",
    "\n",
    "# # remaining lines apply ussual technique of aggregate nad join to selct desired words. We will apply two methods to downsize our 186.000 words dictionary\n",
    "# so we can do reasonable topic modeling with LDA\n",
    "all_value_words_df=pd.DataFrame(all_value_words_array, columns=['wordid','count'])\n",
    "\n",
    "all_value_words_df_count=all_value_words_df.groupby('wordid', as_index=False).count()\n",
    "all_value_words_df_count.columns=['wordid', 'total_docs']\n",
    "\n",
    "all_value_words_df_sum=all_value_words_df.groupby('wordid', as_index=False).sum()\n",
    "all_value_words_df_sum.columns=['wordid', 'total_words']\n",
    "\n",
    "all_value_words_df=pd.merge(all_value_words_df_sum, all_value_words_df_count, how='inner', on=['wordid'])\n",
    "###########\n",
    "low_value_words_df=pd.DataFrame(low_value_words_array, columns=['wordid','count_low'])\n",
    "low_value_words_df=low_value_words_df.groupby('wordid', as_index=False).agg('count')\n",
    "\n",
    "\n",
    "all_value_words_df=pd.merge(all_value_words_df, low_value_words_df, how='left', on=['wordid'])\n",
    "\n",
    "# fix for nan's which appear due to join\n",
    "import math\n",
    "def h(x):\n",
    "    res=0\n",
    "    if not np.isnan(x):\n",
    "        res=int(x)\n",
    "    return res\n",
    "all_value_words_df['count_low']=all_value_words_df['count_low'].apply(lambda x: h(x))\n",
    "all_value_words_df.columns=['wordid', 'total_words', 'total_docs', 'total_docs_low_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In version one of downsizing dictionary (154454 words, which is not feasible for LDA and mpst of these words will be nonsense)\n",
    "# we choose words based on our tfidf analysis.\n",
    "df_words_reduced_tfidf = all_value_words_df[all_value_words_df['total_docs']==all_value_words_df['total_docs_low_tfidf']]\n",
    "\n",
    "# In version two we first take most often arrearing words and then among these we cal again based on number of documents but \n",
    "# choosing least often apearing one.\n",
    "dictionary_size0=all_value_words_df.shape[0] - 10000\n",
    "\n",
    "df_words_reduced_totwords=all_value_words_df.sort('total_words', ascending=1)[0:dictionary_size0]\n",
    "\n",
    "# number of words before and after tfidf based reduction; about 15.000 words kept\n",
    "print all_value_words_df.shape\n",
    "print df_words_reduced_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define list of words to chunk out by our tfidf method\n",
    "low_value_words= list(df_words_reduced_tfidf['wordid'])\n",
    "# update in dictionary\n",
    "dictionary.filter_tokens(bad_ids=low_value_words)\n",
    "# compute new bags of wods list of tupels\n",
    "text_term_matrix = [dictionary.doc2bow(text) for text in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save(fname_or_handle='/notebooks/LDA models and data/Data Frames and lists/Dictionary_tfidf_05.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_term_matrix_clean=[]\n",
    "for i in range(0, len(text_term_matrix)):\n",
    "    if len(np.array(text_term_matrix[i]).shape)==2:\n",
    "        if np.array(text_term_matrix[i]).shape[1]==2:\n",
    "            text_term_matrix_clean.append(text_term_matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('/notebooks/LDA models and data/Data Frames and lists/text_term_matrix2.pkl', 'wb') as pickle_file:\n",
    "    cPickle.dump(obj=text_term_matrix, file=pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
